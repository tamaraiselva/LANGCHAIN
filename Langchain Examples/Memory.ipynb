{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceffc898-b2ed-4566-bf75-d86ae75ce59c",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc7c2c5-b8fd-4d3e-9910-3d32eb0a4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_key import hugging_facehub_key\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hugging_facehub_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679ceb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "# Define LLM\n",
    "repo_id = \"google/flan-t5-large\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61487e09-a239-4a6a-ad0d-a1382c70d43c",
   "metadata": {},
   "source": [
    "# Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857f7632-ed4f-4cfb-90d4-52d7d73bf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abef0aa9-80bd-4d34-acea-30a0ea9df5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat message history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add user and AI messages to the history\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f608f499-195d-4833-8151-5d9ba48eb31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]\n"
     ]
    }
   ],
   "source": [
    "# Print the current messages in the history\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335a2467-4ed9-4ff8-8eb9-1db085b3144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!'), AIMessage(content='whats up?'), HumanMessage(content='Fine, what about you?')]\n"
     ]
    }
   ],
   "source": [
    "# Add another user message and print the updated history\n",
    "history.add_user_message(\"Fine, what about you?\")\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd71b1a-ccb8-4108-bcda-919fbc6485e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template with a question variable\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99b1ae1-1123-45bd-9695-e23fbd11b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create an LLMChain with the prompt template and a language model\n",
    "# Note: 'llm' should be initialized before using it here\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21ab1fa-6483-426d-9394-b4efe8e8d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company's headquarters for our international employees reside in New York City. New York City is a city. New York City is a city. So the answer is New York.\n"
     ]
    }
   ],
   "source": [
    "# Define a question and run the chain to get the model's response\n",
    "question = \"Which city does the company's headquarters for our international employees reside in?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946fe45",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65b5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120237d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is SAM\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SAM: Hello, how can I help you?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is SAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270a2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is SAM\n",
      "AI: SAM: Hello, how can I help you?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SAM: 1+1 is a number that is a combination of two numbers.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035b10cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is SAM\n",
      "AI: SAM: Hello, how can I help you?\n",
      "Human: What is 1+1?\n",
      "AI: SAM: 1+1 is a number that is a combination of two numbers.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SAM: My name is SAM.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd4e7503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is SAM\n",
      "AI: SAM: Hello, how can I help you?\n",
      "Human: What is 1+1?\n",
      "AI: SAM: 1+1 is a number that is a combination of two numbers.\n",
      "Human: What is my name?\n",
      "AI: SAM: My name is SAM.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b5f2dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is SAM\\nAI: SAM: Hello, how can I help you?\\nHuman: What is 1+1?\\nAI: SAM: 1+1 is a number that is a combination of two numbers.\\nHuman: What is my name?\\nAI: SAM: My name is SAM.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30ad520",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e70573ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "624b7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18532465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a24ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68fc08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c222df",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03fcc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8122aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b25044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d577279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fde141d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "263cce10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andrew: Hello, how can I help you?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6342cfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andrew: 1+1 is a number that is a combination of two numbers.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1937d65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andrew: Andrew is a fictional character.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95fb33",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dce97d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2fb637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cff4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c620453",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94014668",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e884ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56870c97-48df-4f5b-a517-35990fc51bb2",
   "metadata": {},
   "source": [
    "## Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e26a1-c1d0-4414-a479-e7285337dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import HuggingFaceHub\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ee082-c94e-4598-a833-dfa4280c9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf670eb-0e3e-4266-9e53-1b68202fb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"google/flan-t5-large\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466d47e-a828-46ac-9136-de4719e17e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215ac2c-11d0-442d-b43e-31f4af226f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    prompt=prompt,\n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23385c-0d22-41f3-830e-6b3a62bcd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_kg.predict(input=\"Hi there! I am Sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb3bb8-0cc3-4a84-9624-bf9c49c10aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_kg.predict(input=\"My TV is broken and I need some customer assistance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4c90d-a134-4e84-99df-e5302edb1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_kg.predict(input=\"My TV is LED sunsung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89f3a3-7c0e-4306-962a-0c9f7e9baa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_kg.predict(input=\"Yes it is and it is still under warranty. my warranty number is A512423\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be72da9-5f87-4474-af28-8f58a2aded0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(conversation_with_kg.memory.kg, with_labels=True)\n",
    "# plt.show()\n",
    "\n",
    "print(conversation_with_kg.memory.kg)\n",
    "print(conversation_with_kg.memory.kg.get_triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dc3f2-2abf-4c61-bf32-41c0aaa83002",
   "metadata": {},
   "source": [
    "## Entity Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ada642-6e25-4349-90d7-3e24610dfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain import HuggingFaceHub\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31ce28-db9b-4b27-8efd-676fc50f49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_MEMORY_CONVERSATION_TEMPLATE.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ebf5e-1074-4603-9d53-14886b66b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "repo_id = \"google/flan-t5-large\"  # The ID of the pre-trained model on Hugging Face\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31f7a8-35a5-4b7c-b06d-ce0fb6a5b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationEntityMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a88006-d7ac-4056-858e-0e96d24879e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8d200-fe2d-4961-bb9a-a2de711aa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi I am Sam. My TV is broken but it is under warranty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adc220-3fee-42d0-87c2-f28f3b179796",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"How can I get it fixed. The warranty number is A512453\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cf236-c224-49a0-ba3d-246a4aa66c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory.entity_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe4799-d60d-4eed-b1af-f386f9b6fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Can you send the repair person call Dave to fix it?.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55746e84-91aa-47ad-9743-8c94907f1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory.entity_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b2221-1716-440c-a2cf-d7993085936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conversation.memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
